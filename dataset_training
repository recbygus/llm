!pip install transformers datasets huggingface_hub


# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2")
model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")

from transformers import AutoTokenizer, AutoModelForCausalLM

# Load model and tokenizer
model_name = "gpt2"  # Change to the model you want
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Add padding token
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

# Load the model
model = AutoModelForCausalLM.from_pretrained(model_name)

# Resize the model's token embeddings since we've added a padding token
model.resize_token_embeddings(len(tokenizer))


from datasets import load_dataset

# Load the dataset
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")

# Split the dataset into training and validation sets
dataset = dataset["train"].train_test_split(test_size=0.1)  # 10% for validation
train_data = dataset["train"]
eval_data = dataset["test"]

# Tokenize the dataset
tokenized_train_data = train_data.map(tokenize_function, batched=True)
tokenized_eval_data = eval_data.map(tokenize_function, batched=True)


from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

training_args = TrainingArguments(
    output_dir="./results",       # Directory where the model will be saved
    num_train_epochs=3,           # Number of epochs
    per_device_train_batch_size=4,# Batch size
    evaluation_strategy="epoch",  # Evaluate at the end of each epoch
    logging_dir='./logs',         # Logging directory
    save_steps=500,               # Save model every 500 steps
    fp16=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_data,
    eval_dataset=tokenized_eval_data,  # Eval
    data_collator=data_collator,
)


# Check if GPU is availble
import torch
print(torch.cuda.is_available())

trainer.train()


eval_results = trainer.evaluate()
print(f"Evaluation results: {eval_results}")


# Save the model and tokenizer locally
model.save_pretrained("./fine_tuned_model")
tokenizer.save_pretrained("./fine_tuned_model")


# Push the fine-tuned model to Hugging Face Hub
model.push_to_hub("gpt2_wikitext_3ep")
tokenizer.push_to_hub("gpt2_wikitext_3ep")

from transformers import pipeline

# Load the fine-tuned model from Hugging Face
model_name = "recbygus/gpt2_wikitext_3ep"
generator = pipeline("text-generation", model=model_name, device=0)  # Use GPU if available

# Generate text with the model
input_text = "When you look at the seas"
output = generator(input_text, max_length=50, num_return_sequences=1, truncation=True)

# Print the generated text
print(output[0]['generated_text'])


!rm -rf /results
